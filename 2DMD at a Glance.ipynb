{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b55e377",
   "metadata": {},
   "source": [
    " # 2DMD at a Glance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885fdff3",
   "metadata": {},
   "source": [
    "The commands below import the libraries used throughout the Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73bec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import gzip\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import chain\n",
    "from joblib import Parallel, delayed\n",
    "from pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n",
    "from pymatgen.core import Structure\n",
    "from pymatgen.core.periodic_table import Element\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af9e8e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9dc537",
   "metadata": {},
   "source": [
    "Functions for data processing and data preparation in the Pandas DataFrame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d47806",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASES_and_CONCENTRATIONS = [['BP_spin_500', 'high'], ['GaSe_spin_500', 'high'],\n",
    "                            ['hBN_spin_500', 'high'], ['InSe_spin_500', 'high'],\n",
    "                            ['MoS2', 'low'], ['MoS2_500', 'high'],\n",
    "                            ['WSe2', 'low'], ['WSe2_500', 'high']\n",
    "                           ]\n",
    "\n",
    "\n",
    "def readStructures(base, concentration, structure_ids):\n",
    "    \n",
    "    with zipfile.ZipFile('2d-materials-point-defects-all.zip') as datasets_zip:\n",
    "        \n",
    "        base_in_zip = f'{concentration}_density_defects/{base}'\n",
    "        with tarfile.open(\n",
    "            fileobj=io.BytesIO(datasets_zip.read(f'{base_in_zip}/initial.tar.gz'))\n",
    "        ) as tf:\n",
    "            cifs = [tf.extractfile(f'{_id}.cif').read().decode(\"utf-8\") \n",
    "                    for _id in tqdm(structure_ids)]\n",
    "\n",
    "    return pd.DataFrame(cifs, index=structure_ids, columns=['cif'])\n",
    "\n",
    "def parseStructure(cif_string):\n",
    "\n",
    "    structure = Structure.from_str(cif_string, fmt='cif') #.get_structures(primitive=False)[0]\n",
    "    space_group_number = SpacegroupAnalyzer(structure).get_space_group_number()\n",
    "    lattice = structure.lattice.matrix\n",
    "    positions = np.array([x.coords for x in structure])\n",
    "    atomic_symbols = np.array(list(map(lambda x: x.symbol, structure.species)))\n",
    "    atomic_numbers = np.array(list(map(lambda x: Element(x).number, atomic_symbols)))\n",
    "    formula = OrderedDict(sorted(Counter(atomic_symbols).items()))\n",
    "    return [space_group_number, lattice, positions, atomic_symbols, atomic_numbers, formula, \n",
    "                         len(atomic_numbers), len(formula)]\n",
    "\n",
    "\n",
    "def structureFormationEnergy(structure, neat_elements):\n",
    "\n",
    "    fe = structure['energy']\n",
    "    formula = structure['formula']\n",
    "    for element in formula.keys() :\n",
    "        element_chem_potential = neat_elements.loc[element]['chemical_potential']\n",
    "        fe -= formula[element] * element_chem_potential\n",
    "    return fe\n",
    "\n",
    "\n",
    "def flatList(list_):\n",
    "    \n",
    "    return [str(item) for sublist in list_ for item in sublist]\n",
    "\n",
    "\n",
    "def readDataFromArchive(base, concentration):\n",
    "\n",
    "    with zipfile.ZipFile('2d-materials-point-defects-all.zip') as datasets_zip:\n",
    "\n",
    "        base_in_zip = f'{concentration}_density_defects/{base}'\n",
    "\n",
    "        with datasets_zip.open(f'{base_in_zip}/defects.csv.gz') as defects_gz :\n",
    "            with gzip.open(defects_gz) as defects_data :\n",
    "                defects_df = pd.read_csv(defects_data, sep=',')\n",
    "\n",
    "        with datasets_zip.open(f'{base_in_zip}/descriptors.csv') as descriptors :\n",
    "            descriptors_df = pd.read_csv(descriptors, sep=',')\n",
    "\n",
    "        with datasets_zip.open(f'{base_in_zip}/elements.csv') as elements :\n",
    "            neat_elements_df = pd.read_csv(elements, sep=',')\n",
    "        neat_elements_df.set_index('element', inplace=True, drop=True)\n",
    "        print(neat_elements_df)\n",
    "\n",
    "        with datasets_zip.open(f'{base_in_zip}/initial_structures.csv') as init_structures :\n",
    "            init_structures_df = pd.read_csv(init_structures, sep=',')\n",
    "\n",
    "        with datasets_zip.open(f'{base_in_zip}/targets.csv.gz') as targets_gz :\n",
    "            with gzip.open(targets_gz) as targets_data :\n",
    "                targets_df = pd.read_csv(targets_data, sep=',')\n",
    "\n",
    "    return defects_df, descriptors_df, neat_elements_df, init_structures_df, targets_df\n",
    "\n",
    "\n",
    "def handleTableData(base, concentration) :\n",
    "\n",
    "    defects_df, descriptors_df, neat_elements_df, init_structures_df, targets_df = \\\n",
    "                                                        readDataFromArchive(base, concentration)\n",
    "    print('\\n' + '*'*90 + '\\n')\n",
    "    \n",
    "    print(f'Information on defects.csv.gz for {base}_{concentration}_concentration')\n",
    "    for unused_property in ['Unnamed: 0', 'defect_id'] :\n",
    "        if unused_property in defects_df.columns:\n",
    "            defects_df.drop(unused_property, axis=1, inplace=True)\n",
    "    print('\\tcolumns:', defects_df.columns)\n",
    "    print('\\tshape:', defects_df.shape)\n",
    "    number_of_unique = len(defects_df['_id'].unique())\n",
    "    print(f'\\t{number_of_unique} unique defect structures')\n",
    "    number_of_unique = len(defects_df['descriptor_id'].unique())\n",
    "    print(f'\\t{number_of_unique} unique defect descriptors')\n",
    "\n",
    "    print(f'Information on descriptors.csv for {base}_{concentration}_concentration')\n",
    "    for unused_property in ['Unnamed: 0', 'defect_id'] :\n",
    "        if unused_property in descriptors_df.columns:\n",
    "            descriptors_df.drop(unused_property, axis=1, inplace=True)\n",
    "    print('\\tcolumns:', descriptors_df.columns)\n",
    "    print('\\tshape:', descriptors_df.shape)\n",
    "    number_of_unique = len(descriptors_df['_id'].unique())\n",
    "    print(f'\\t{number_of_unique} unique defect descriptors')\n",
    "\n",
    "    print(f'Information on targets.csv for {base}_{concentration}_concentration')\n",
    "    for unused_property in ['Unnamed: 0', 'defect_id'] :\n",
    "        if unused_property in targets_df.columns:\n",
    "            targets_df.drop(unused_property, axis=1, inplace=True)\n",
    "    print('\\tcolumns:', targets_df.columns)\n",
    "    print('\\tshape:', targets_df.shape)\n",
    "    number_of_unique = len(targets_df['_id'].unique())\n",
    "    print(f'\\t{number_of_unique} unique defect structures')\n",
    "    number_of_unique = len(targets_df['descriptor_id'].unique())\n",
    "    print(f'\\t{number_of_unique} unique defect descriptors')\n",
    "\n",
    "    print('\\n' + '*'*90 + '\\n')\n",
    "    print('Data checking...')\n",
    "    print(f'Is \\'descriptor_id\\'s (defects.csv) a subset of \\'_id\\'s (descriptors.csv)?', \n",
    "          set(defects_df['descriptor_id']).issubset(set(descriptors_df['_id'])))\n",
    "\n",
    "    defects_df.set_index('_id', inplace=True, drop=True)\n",
    "    targets_df.set_index('_id', inplace=True, drop=True)\n",
    "    descriptors_df.set_index('_id', inplace=True, drop=True)        \n",
    "\n",
    "    data = pd.DataFrame(descriptors_df.loc[defects_df['descriptor_id']].values, \n",
    "                                    columns=descriptors_df.columns, index=defects_df.index)\n",
    "\n",
    "    for additional_data in [defects_df, targets_df] :\n",
    "        for piece_of_data in additional_data.columns :\n",
    "            if (piece_of_data in data.columns) and (additional_data[piece_of_data].dtype != 'O') :\n",
    "                diff = additional_data[piece_of_data] - data[piece_of_data]\n",
    "                out_ = np.abs(diff).max()\n",
    "                print(f'{piece_of_data:>21s} is in the data with the maximum difference of {out_}')\n",
    "            elif (piece_of_data in data.columns) and (additional_data[piece_of_data].dtype == 'O') :\n",
    "                out_ = all(data[piece_of_data] == additional_data[piece_of_data])\n",
    "                print(f'{piece_of_data:>21s} is in the data. Are the column contents the same? {out_}')\n",
    "            else:\n",
    "                data[piece_of_data] = additional_data[piece_of_data]\n",
    "\n",
    "    print('\\n' + '*'*90 + '\\n')\n",
    "    cells_available = sorted(data['cell'].unique())\n",
    "    print('Unique supercells:', cells_available)\n",
    "    print('\\n' + '*'*90 + '\\n')\n",
    "    \n",
    "    data['defects'] = data['defects'].apply(ast.literal_eval)\n",
    "    data['defect_nsites'] = data['defects'].apply(len)\n",
    "                    \n",
    "    return (data, neat_elements_df, init_structures_df)\n",
    "\n",
    "\n",
    "def prepareBaseConcentrationDataset(base, concentration, save_to_file=False) :\n",
    "\n",
    "    data, neat_elements, _ = handleTableData(base, concentration)\n",
    "\n",
    "    print('Reading structures...')\n",
    "    structures = readStructures(base, concentration, data.index)\n",
    "    data = pd.concat((data, structures), axis=1)\n",
    "\n",
    "    # parse structural info\n",
    "    print('Parsing structural information...')\n",
    "    additional_data = Parallel(n_jobs=-1)(delayed(parseStructure)\\\n",
    "                                     (data.loc[_id].cif) \n",
    "                                     for _id in tqdm(data.index))\n",
    "    additional_data = pd.DataFrame(additional_data, index=data.index, \n",
    "                        columns=['space_group_no', 'lattice', 'atomic_positions', 'atomic_symbols', \n",
    "                                           'atomic_numbers', 'formula', 'nsites', 'nspecies'])\n",
    "\n",
    "    data = pd.concat((data, additional_data), axis=1)\n",
    "\n",
    "    # collect additional data\n",
    "    data['defect_concentration'] = concentration\n",
    "    data['structure_formation_energy'] = [structureFormationEnergy(data.loc[i], neat_elements) \n",
    "                                             for i in data.index]\n",
    "    data['structure_formation_energy_per_atom'] = data['structure_formation_energy'] / data['nsites']\n",
    "    data['composition_string'] = data['formula'].apply(lambda x: '_'.join(flatList(list(x.items()))))\n",
    "\n",
    "    data = data[sorted(data.columns)]\n",
    "    \n",
    "    if save_to_file :\n",
    "        data.to_pickle(f'2d-base_{base}-{concentration}_defect_concentration-table.pkl.gz')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def prepareFullDataset(save_to_file=True, filename='2d-materials-point-defects-all-table.pkl.gz'):\n",
    "\n",
    "    base_datasets = [prepareBaseConcentrationDataset(base, concentration) \n",
    "                         for base, concentration in BASES_and_CONCENTRATIONS]\n",
    "    \n",
    "    full_dataset = pd.concat(base_datasets, axis=0)\n",
    "\n",
    "    full_dataset['ordinal_id'] = list(map(float, range(full_dataset.shape[0])))\n",
    "    full_dataset = full_dataset[sorted(full_dataset.columns)]\n",
    "    print(f'Full dataset shape is {full_dataset.shape}')\n",
    "    \n",
    "    if save_to_file:\n",
    "        full_dataset.to_pickle(filename)\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d32357",
   "metadata": {},
   "source": [
    "## Download and Processing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf5e563",
   "metadata": {},
   "source": [
    "This part serves to download the dataset and transform it into a pandas dataframe.\n",
    "\n",
    "The code of the cell below can be executed <ins>once</ins> and will result in two files being created in the folder it was run from, namely *'2d-materials-point-defects-all.zip'* (the original data) and *'2d-materials-point-defects-all-table.pkl.gz'* (data in table format for further use). \n",
    "On a MacBook Pro (Intel Core i7 12 threads), data collection takes about 3 minutes for the first run.\n",
    "On subsequent runs, the cell reports the presence of the data in table format and does not repeat processing (if necessary, delete the file and repeat processing).\n",
    "\n",
    "**Reminder**: If you use the 2DMD dataset, please cite the following paper:\n",
    "\n",
    "Huang, P., Lukin, R., Faleev, M. et al. Unveiling the complex structure-property correlation of defects in 2D materials based on high throughput datasets. npj 2D Mater Appl 7, 6 (2023). https://doi.org/10.1038/s41699-023-00369-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149e139",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.isfile('2d-materials-point-defects-all-table.pkl.gz') :\n",
    "    url_main = 'https://constructor.app/platform/open/2d-materials-point-defects/attachments\n",
    "    url_attach = 'b62e17e599364a12941a9ae494a14736?name=2d-materials-point-defects-all.zip'\n",
    "    request = requests.get('/'.join([url_main, url_attach]), allow_redirects=True)\n",
    "    open('2d-materials-point-defects-all.zip', 'wb').write(request.content)\n",
    "    print('\\nThe dataset has been successfully downloaded and is currently being processed!\\n')\n",
    "\n",
    "    prepareFullDataset()\n",
    "    \n",
    "else :\n",
    "    print('\\nThe dataset was downloaded and processed previously!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a889c4f",
   "metadata": {},
   "source": [
    "## Handle data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1c30e",
   "metadata": {},
   "source": [
    "### fast screening routines\n",
    "quick procedures are aimed at obtaining general information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa92de",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.read_pickle('2d-materials-point-defects-all-table.pkl.gz')\n",
    "print(f'Dataset shape (objects, features) is {full_dataset.shape}')\n",
    "print(f'Number of unique structure ids is {len(full_dataset.index.unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed6d75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda2878",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a381aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.groupby(['base', 'cell', 'defect_concentration', 'defect_nsites',]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb22dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(full_dataset[['structure_formation_energy',\n",
    "                           'structure_formation_energy_per_atom',\n",
    "                           'nsites', 'defect_nsites', \n",
    "                           'homo_lumo_gap_max', 'homo_lumo_gap_min',\n",
    "                           'base']], \n",
    "                         hue='base', palette='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440b7b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sub_data = full_dataset[(full_dataset['base'] == 'WSe2') | (full_dataset['base'] == 'MoS2')]\n",
    "sns.pairplot(sub_data[['formation_energy', 'structure_formation_energy',\n",
    "                       'nsites', 'defect_nsites', 'base']], \n",
    "             hue='base', palette='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = full_dataset[(full_dataset['base'] != 'WSe2') & (full_dataset['base'] != 'MoS2')]\n",
    "sns.pairplot(sub_data[['formation_energy', 'structure_formation_energy',\n",
    "                       'nsites', 'defect_nsites', 'base']], \n",
    "             hue='base', palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac1659",
   "metadata": {},
   "source": [
    "### defects and space groups\n",
    "the obtained defect/symmetry relations for the WSe<sub>2</sub> and MoS<sub>2</sub> base materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba6e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[full_dataset['base']=='WSe2'].groupby(['defect_nsites','space_group_no']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[full_dataset['base']=='MoS2'].groupby(['defect_nsites','space_group_no']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc911d10",
   "metadata": {},
   "source": [
    "### target analysis\n",
    "visual and numerical analysis of target (thermodynamic) properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16990238",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for base in ['WSe2', 'MoS2'] :\n",
    "    sub_data = full_dataset[(full_dataset['base'] == base) & (full_dataset['defect_nsites'] >= 3)]\n",
    "    sns.pairplot(sub_data[['structure_formation_energy', 'formation_energy', \n",
    "                           'space_group_no','defect_nsites']],\n",
    "                 hue='defect_nsites', palette='flare', corner=True)\n",
    "    # plt.title(base)\n",
    "    plt.show()\n",
    "    sub_data = full_dataset[(full_dataset['base'] == base) & (full_dataset['defect_nsites'] >= 3)]\n",
    "    sns.pairplot(sub_data[['structure_formation_energy_per_atom', 'formation_energy_per_site',\n",
    "                           'space_group_no','defect_nsites']],\n",
    "                 hue='defect_nsites', palette='flare', corner=True)\n",
    "    plt.title(base)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for base in ['WSe2', 'MoS2'] :\n",
    "    result = []\n",
    "    targets_to_analize = ['structure_formation_energy', 'structure_formation_energy_per_atom',\n",
    "                          'formation_energy', 'formation_energy_per_site', ]\n",
    "    defect_contents = sorted(full_dataset[full_dataset['base'] == base]['defect_nsites'].unique())\n",
    "    for defect_ns in defect_contents :\n",
    "        subset = full_dataset[(full_dataset['base'] == base) & \\\n",
    "                                      (full_dataset['defect_nsites'] == defect_ns)][targets_to_analize]\n",
    "        result += [[subset[target].std() for target in targets_to_analize]]\n",
    "\n",
    "    result = np.array(result)\n",
    "    result = result.T\n",
    "\n",
    "    for i in range(result.shape[0]) :\n",
    "        if i == 0 :\n",
    "            plt.plot(result[i],  'o-', lw=2, label=targets_to_analize[i],)\n",
    "        else :\n",
    "            plt.plot(result[i], lw=2, label=targets_to_analize[i])\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('defect positions')\n",
    "    plt.ylabel('std of target property, a.u.')\n",
    "    plt.title(base)\n",
    "    plt.xticks(range(len(defect_contents)), list(map(str, defect_contents)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85492fbc",
   "metadata": {},
   "source": [
    "### Sample input for Graph Neural Networks (Allegro)\n",
    "the code below represents a way to prepare the MoS2 (or alternatively WSe2) subsets for training, validating, and testing a graph neural network and includes visual control of target property distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allegroInput(dataset_file) :\n",
    "    \n",
    "    print(f'Allegro input generation for {dataset_file}')\n",
    "    data = pd.read_pickle(dataset_file)\n",
    "    dataset_xyz = re.sub('pkl.gz', 'xyz', dataset_file)\n",
    "    with open(dataset_xyz, 'w') as out :\n",
    "        for index in tqdm(data.index) :\n",
    "            item = data.loc[index]\n",
    "            lattice = item['lattice']\n",
    "            atomic_symbols = item['atomic_symbols']\n",
    "            positions = item['atomic_positions']\n",
    "            fe = item['structure_formation_energy']\n",
    "            fe_pa = item['structure_formation_energy_per_atom']\n",
    "            fe_ori = item['formation_energy']\n",
    "            ordinal_id = item['ordinal_id']\n",
    "\n",
    "            out.write(f'{item.nsites}\\n')\n",
    "            out.write('Lattice=\\\"'+' '.join(list(map(str, list((lattice).reshape(-1,)))))+'\\\" ')\n",
    "            out.write('Properties=species:S:1:pos:R:3 pbc=\\\"T T F\\\"  ') \n",
    "            out.write(f'ordinal_id={ordinal_id} \\\n",
    "                        structure_formation_energy={fe} \\\n",
    "                        structure_formation_energy_per_atom={fe_pa} \\\n",
    "                        original_formation_energy={fe_ori}')        \n",
    "            out.write('\\n')\n",
    "            for symbol, pos in zip(atomic_symbols, positions) :\n",
    "                out.write(symbol + '   ' + ' '.join(list(map(str, pos)))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77049be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('for_allegro/') :\n",
    "    os.mkdir('for_allegro/')\n",
    "    \n",
    "base = 'MoS2' # base to be processed\n",
    "base_dataset = full_dataset[full_dataset['base'] == base].copy()\n",
    "\n",
    "# selection of high symmetry structures among low defect contents\n",
    "high_sym_base = base_dataset[(base_dataset['defect_nsites'] < 4) & \\\n",
    "                                     (base_dataset['space_group_no'] >= 8)].copy()\n",
    "\n",
    "print('train_val_high\\n', high_sym_base.shape, '\\n',\n",
    "      high_sym_base.groupby(['defect_nsites', 'space_group_no']).size())\n",
    "\n",
    "# train/validation (high symmetry structures) split among low defect contents\n",
    "train_high, val_high = train_test_split(high_sym_base, test_size=0.2, shuffle=True, \n",
    "                              stratify=high_sym_base['defect_nsites'], random_state=0)\n",
    "\n",
    "train_high.to_pickle(f'for_allegro/{base}_high_sym_train.pkl.gz')\n",
    "val_high.to_pickle(f'for_allegro/{base}_high_sym_val.pkl.gz')\n",
    "print()\n",
    "\n",
    "# test subset (250 low symmetry structures) among low defect contents\n",
    "low_sym_base = base_dataset[(base_dataset['defect_nsites'] < 4) & \\\n",
    "                            (base_dataset['space_group_no'] < 8)].copy()\n",
    "_, test_low = train_test_split(low_sym_base, test_size=250, shuffle=True, \n",
    "                              stratify=low_sym_base['defect_nsites'], random_state=0)\n",
    "test_low.to_pickle(f'for_allegro/{base}_low_def_test.pkl.gz')\n",
    "\n",
    "print('test_low\\n', test_low.shape, '\\n',\n",
    "      test_low.groupby(['defect_nsites', 'space_group_no']).size())\n",
    "\n",
    "# test subset (250 low symmetry structures) among high defect contents\n",
    "high_def_cont = base_dataset[base_dataset['defect_nsites'] >= 4].copy()\n",
    "_, test_high = train_test_split(high_def_cont, test_size=250, shuffle=True, \n",
    "                          stratify=high_def_cont['defect_nsites'], random_state=0)\n",
    "test_high.to_pickle(f'for_allegro/{base}_high_def_test.pkl.gz')\n",
    "\n",
    "print('test_high\\n', test_high.shape, '\\n',\n",
    "      test_high.groupby(['defect_nsites', 'space_group_no']).size())\n",
    "\n",
    "print('test.shape', pd.concat((test_low, test_high)).shape)\n",
    "print()\n",
    "\n",
    "# train/validation (low symmetry structures) split among low defect contents\n",
    "# the structures comprising the test subsets above are subtracted from traininig/validation ones\n",
    "not_test_data = low_sym_base[~np.isin(low_sym_base.index, test_low.index)]\n",
    "not_test_data = not_test_data.sample(n=high_sym_base.shape[0], random_state=0)\n",
    "\n",
    "print('train_val_low\\n', not_test_data.shape, '\\n',\n",
    "      not_test_data.groupby(['defect_nsites', 'space_group_no']).size())\n",
    "train_low, val_low = train_test_split(not_test_data, test_size=0.2, shuffle=True, \n",
    "                              stratify=not_test_data['defect_nsites'], random_state=0)\n",
    "\n",
    "train_low.to_pickle(f'for_allegro/{base}_low_sym_train.pkl.gz')\n",
    "val_low.to_pickle(f'for_allegro/{base}_low_sym_val.pkl.gz')\n",
    "print()\n",
    "\n",
    "\n",
    "for energy_type in ['structure_formation_energy_per_atom', 'structure_formation_energy',\n",
    "                    'formation_energy'] :\n",
    "    per_atom_addon = '/atom' if energy_type.endswith('_per_atom') else ''\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 3))\n",
    "    plt.suptitle(f'TEST: Energy type: {energy_type}')\n",
    "    \n",
    "    axs[0].hist([data[energy_type] for data in [train_high, val_high, test_low, test_high]], \n",
    "                label=['train', 'val', 'test_low', 'test_high'])\n",
    "    axs[0].set_xlabel(f'{energy_type}, eV{per_atom_addon}')\n",
    "    axs[0].set_ylabel('Number of structures')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(alpha=0.3)\n",
    "    axs[0].set_title('High symmetry train/val')\n",
    "\n",
    "    axs[1].hist([data[energy_type] for data in [train_low, val_low, test_low, test_high]])\n",
    "    axs[1].set_xlabel(f'{energy_type}, eV{per_atom_addon}')\n",
    "    axs[1].grid(alpha=0.3)\n",
    "    axs[1].set_title('Low symmetry train/val')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# convert subsets of data to xyz format\n",
    "allegroInput(dataset_file=f'for_allegro/{base}_high_sym_train.pkl.gz')\n",
    "allegroInput(dataset_file=f'for_allegro/{base}_high_sym_val.pkl.gz')\n",
    "allegroInput(dataset_file=f'for_allegro/{base}_low_sym_train.pkl.gz')\n",
    "allegroInput(dataset_file=f'for_allegro/{base}_low_sym_val.pkl.gz')\n",
    "allegroInput(dataset_file=f'for_allegro/{base}_low_def_test.pkl.gz')\n",
    "allegroInput(dataset_file=f'for_allegro/{base}_high_def_test.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b0ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
